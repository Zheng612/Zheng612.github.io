<!DOCTYPE html>
<html lang="">

<head>
  <title>üìò Heart disease prediction | </title>

  <meta charset="UTF-8">
  <meta name="language" content="en">
  <meta name="description" content="">
  <meta name="keywords" content="Project">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  
  

  <link rel="shortcut icon" type="image/png" href="/favicon.ico" />

  
  
    
 
  
  
  
  
  
  
    
    <link type="text/css" rel="stylesheet" href="/css/post.min.8a92e8cd796cd68e81df831649d4a7ddee5f9c2504a95f5b93ef40764b3eaab3.css" integrity="sha256-ipLozXls1o6B34MWSdSn3e5fnCUEqV9bk&#43;9Adks&#43;qrM="/>
  
    
    <link type="text/css" rel="stylesheet" href="/css/custom.min.445d2c3db5a9b9fa38e9de7ebb59566c83ae827174902a293925b67bdaf1c589.css" integrity="sha256-RF0sPbWpufo46d5&#43;u1lWbIOugnF0kCopOSW2e9rxxYk="/>
  
  
   
   
    

<script type="application/ld+json">
  
    {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": ""
      },
      "articleSection" : "blog",
      "name" : "üìò Heart disease prediction",
      "headline" : "üìò Heart disease prediction",
      "description" : "",
      "inLanguage" : "en-US",
      "author" : "",
      "creator" : "",
      "publisher": "",
      "accountablePerson" : "",
      "copyrightHolder" : "",
      "copyrightYear" : "2020",
      "datePublished": "2020-10-10 21:56:06 -0500 CDT",
      "dateModified" : "2020-10-10 21:56:06 -0500 CDT",
      "url" : "\/blog\/chd\/",
      "wordCount" : "911",
      "keywords" : ["Project", "Blog"]
    }
  
  </script>
</head>

<body>
  <div class="burger__container">
  <div class="burger" aria-controls="navigation" aria-label="Menu">
    <div class="burger__meat burger__meat--1"></div>
    <div class="burger__meat burger__meat--2"></div>
    <div class="burger__meat burger__meat--3"></div>
  </div>
</div>
 

  <nav class="nav" id="navigation">
  <ul class="nav__list">
    
    
      <li>
        <a  href="/about">about</a>
      </li>
    
      <li>
        <a  class="active"
         href="/blog">blog</a>
      </li>
    
      <li>
        <a  href="/coding_practice">coding practice</a>
      </li>
    
      <li>
        <a  href="/contact">contact</a>
      </li>
    
      <li>
        <a  href="/">home</a>
      </li>
    
  </ul>
</nav>


  <main>
    
    

    <div class="flex-wrapper">
      <div class="post__container">
        <div class="post">
          <header class="post__header">
            <h1 id="post__title">üìò Heart disease prediction</h1>
            <time datetime="2020-10-10 21:56:06 -0500 CDT" class="post__date">Oct 10 2020</time> 
          </header>
          <article class="post__content">
              
<p><img src="/images/sign.jpeg" alt="Alt text for my gif"></p>
<h2 id="introduction">Introduction<a class="anchor" href="#introduction">#</a></h2>
<p>Coronary heart disease (CHD) is one of the most common types of heart disease in America. Making early prognosis can help high risk potential patients to change lifesyle and reduce the chance of getting it. The objective of the project is to determine the risk factors for predicting the probability of developing CHD.</p>
<p>The ‚Äò<a href="https://www.kaggle.com/amanajmera1/framingham-heart-study-dataset#framingham.csv">framingham</a>‚Äô dataset provides information whether the participants developed Coronary Heart Disease (CHD) after ten years of the study.</p>
<h2 id="data">Data<a class="anchor" href="#data">#</a></h2>
<p>Now let&rsquo;s import the data. The dataset includes 4,240 samples with a total of 16 columns, consisting of eight categorical and eight numerical attributes.</p>
<pre><code class="language-json">data=pd.read_csv(&quot;framingham.csv&quot;)
data.isnull().sum()
</code></pre>
<pre><code>male 0 
age 0 
education 105 
currentSmoker 0 
cigsPerDay 29 
BPMeds 53 
prevalentStroke 0 
prevalentHyp 0 
diabetes 0 
totChol 50 
sysBP 0 
diaBP 0 
BMI 19 
heartRate 1 
glucose 388 
TenYearCHD 0
</code></pre>
<h3 id="replace-missing-value-with-most-frequent-value---education-column">Replace missing value with most frequent value - Education Column<a class="anchor" href="#replace-missing-value-with-most-frequent-value---education-column">#</a></h3>
<p>Among the seven  features with missing values, there are two categorical features: education and BPMeds. Missing values of these two categorical features are replaced by ‚Äúthe most frequent values‚Äù within each column. Take the education column as an example:</p>
<pre><code class="language-json"># find the most frequent value
data['education'].value_counts()
</code></pre>
<table>
<thead>
<tr>
<th>missing</th>
<th>value_counts</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.0</td>
<td>1720</td>
</tr>
<tr>
<td>2.0</td>
<td>1253</td>
</tr>
<tr>
<td>3.0</td>
<td>689</td>
</tr>
<tr>
<td>4.0</td>
<td>473</td>
</tr>
</tbody>
</table>
<pre><code class="language-json">#replace missing values with &quot;1.0&quot;
def replace_most_common(x): 
	if pd.isnull(x):
		return 1.0 
	else:
		return x
data['education'] = data['education'].map(replace_most_common)
</code></pre>
<h3 id="replace-missing-values-with-median---cigsperday">Replace missing values with median - cigsPerDay<a class="anchor" href="#replace-missing-values-with-median---cigsperday">#</a></h3>
<p>For the five numerical features (cigsPerDay, totChol, BMI, glucose, HeartRate), the missing values are replaced with the median value since due to asymmetrical distributions in the histograms. Take cigsPerDay as an example:</p>
<pre><code>plt.hist(data['cigsPerDay'], alpha=0.5)
</code></pre>
<p><img src="/images/chd3.png" alt="this is an image"></p>
<pre><code>data[&quot;cigsPerDay&quot;].fillna(data['cigsPerDay'].median(),inplace=True)
</code></pre>
<h2 id="feature-engineering---backward-elimination-p-value">Feature Engineering - Backward elimination (p-value)<a class="anchor" href="#feature-engineering---backward-elimination-p-value">#</a></h2>
<p>Feature selection technique is often used in Machine Learning as a strategy to include variables from a dataset. The fundamental idea of feature selection is to fit the features that are statistically most relevant to the model.</p>
<p>Backward elimination is decided on a significant level for a variable to be retained in the model using a p-value. First, a model is fitted with all independent variables. Then the variable with the highest p-values is removed before the model is fitted again with the removed variable. The process continues until no further steps are necessary.</p>
<p>Before we begin with Backward elimination, we need to append ‚Äò1‚Äô at the beginning of our data set. Now, why is this important? For performing Backward elimination, we are required to use the linear model provided by statsmodels library ‚Äî which does not consider the bias term. Hence, by adding a dummy feature with value as ‚Äò1‚Äô, our equation becomes y=b.x0+m1.x1+m2.x2+m3.x3+m4.x4 where x0 = 1.</p>
<pre><code class="language-json">X.shape
[output]: (4240,15)
</code></pre>
<p>Append column of ones:</p>
<pre><code class="language-json">#append column of ones
X_with_ones= np.append (arr=np.ones([X.shape[0],1]).astype(int), values = X, axis = 1) 
print(&quot;before Elimination, there are&quot;,X_with_ones.shape[1],&quot;columns&quot;)

[output]:before Elimination, there are 16 columns
</code></pre>
<pre><code class="language-json">import statsmodels.api as sm 
def backwardElimination(x, sl):
	numVars= len(x[0])
	for i in range(0, numVars):
		obj_OLS= sm.OLS(y, x).fit()
		maxVar= max(obj_OLS.pvalues).astype(float) if maxVar&gt; sl:
			for j in range(0, numVars-i):
				if (obj_OLS.pvalues[j].astype(float) == maxVar):
					x = np.delete(x, j, 1) 
	obj_OLS.summary()
	return x
</code></pre>
<pre><code class="language-json">SL=0.05
X_modeled=backwardElimination(X_with_ones, SL)
X_modeled.shape
print(&quot;after backward elimination, there are 7 columns left.&quot;)

</code></pre>
<p><img src="/images/chd4.png" alt="this is an image"></p>
<h2 id="logistic-regression">Logistic Regression:<a class="anchor" href="#logistic-regression">#</a></h2>
<p>After splitting the data to train and test, we can start to fire the models. In this post, I will take logistic regression as an example:</p>
<p>Despite having ‚Äúregression‚Äù in its name, logistic regression is one of the most widely used techniques in machine learning to solve classification problems. It can be used when the dependent variable is categorical.</p>
<pre><code class="language-jsonlogreg">predict_logreg=logreg.predict(X_test)
Logistic_regression_score=logreg.score(X_test, y_test)*100
print(Logistic_regression_score)

[output]:86.0377358490566
</code></pre>
<h2 id="k-fold">K-FOLD:<a class="anchor" href="#k-fold">#</a></h2>
<p>K-Fold CV method splits the training dataset into K number of folds, where K-1 folds are used for the model training, and one fold is used for performance evaluation. The procedure is repeated K times for K number of models to obtain performance estimates. A good standard value for K is 10 based on empirical evidence. Thus, K is set to be 10 for this project as well.</p>
<pre><code>def run_kfold(logreg):
	kf = KFold(n_splits=10)
	outcomes = []
	fold = 0
	for train_index, test_index in kf.split(X_modeled):
		fold += 1
		X_train, X_test = X_modeled[train_index], X_modeled[test_index] 
		y_train, y_test = y[train_index], y[test_index] 
		logreg.fit(X_train, y_train)
		accuracy_lr = logreg.score(X_test, y_test)*100 
		outcomes.append(accuracy_lr)
		print(&quot;Fold {0} accuracy: {1}&quot;.format(fold, accuracy_lr))
	mean_outcome_logistic_regression = np.mean(outcomes)
	print(&quot;Mean Accuracy: {0}&quot;.format(mean_outcome_logistic_regression)) 
	return(mean_outcome_logistic_regression)
logreg_Kfoldscore=run_kfold(logreg)
</code></pre>
<pre><code>[output]:
Fold 1 accuracy: 81.36792452830188
Fold 2 accuracy: 85.84905660377359
Fold 3 accuracy: 85.84905660377359
Fold 4 accuracy: 87.97169811320755
Fold 5 accuracy: 85.84905660377359
Fold 6 accuracy: 83.9622641509434
Fold 7 accuracy: 86.55660377358491
Fold 8 accuracy: 86.08490566037736
Fold 9 accuracy: 86.08490566037736
Fold 10 accuracy: 83.9622641509434
Mean Accuracy: 85.35377358490565
</code></pre>
<h2 id="confusion-matrix---logistic-regression">Confusion Matrix - logistic regression:<a class="anchor" href="#confusion-matrix---logistic-regression">#</a></h2>
<pre><code>results_lr = confusion_matrix(y_test, predict_logreg) print(results_lr) ax=sns.heatmap(results_lr,linewidths=1,vmax=1000,
square=True, cmap=&quot;Blues&quot;,annot=True)
#if i don't run the next 2 lines of code, the heatmap will be cut off. bottom, top = ax.get_ylim()
ax.set_ylim(bottom + 0.5, top - 0.5)
</code></pre>
<p><img src="/images/chd5.png" alt="this is an image"></p>
<p>Although this post only take logistic regression as an example, but other models were also being tested. Here are the results:</p>
<pre><code>models = pd.DataFrame({
'Model': ['Support Vector Machines', 'Logistic Regression','KNN',
              'Decision Tree','Naive Bayes', 'Random Forest',
              ],
    'Score': [SVM_score, Logistic_regression_score,knn_score, decision_tree_score, NB_score,
              RandomForest_score],
    'Kfold score': [svc_KFoldScore,logreg_Kfoldscore,knn_Kfoldscore,DT_Kfoldscore, nb_Kfoldscore,
rf_Kfoldscore]}) models.sort_values(by='Score', ascending=False)
</code></pre>
<table>
<thead>
<tr>
<th>Model</th>
<th>Score</th>
<th>Kfold score</th>
</tr>
</thead>
<tbody>
<tr>
<td>Logistic Regression</td>
<td>86.037736</td>
<td>85.353774</td>
</tr>
<tr>
<td>Support Vector Machines</td>
<td>85.000000</td>
<td>84.481132</td>
</tr>
<tr>
<td>Naive Bayes</td>
<td>84.716981</td>
<td>84.410377</td>
</tr>
<tr>
<td>Random Forest</td>
<td>83.867925</td>
<td>83.066038</td>
</tr>
<tr>
<td>KNN</td>
<td>81.415094</td>
<td>82.075472</td>
</tr>
<tr>
<td>Decision Tree</td>
<td>74.905660</td>
<td>75.683962</td>
</tr>
</tbody>
</table>
<h2 id="conclusion">Conclusion<a class="anchor" href="#conclusion">#</a></h2>
<p>The model testing presents different results based on the two approaches, feature selection and feature extraction. With feature selection, gender (Male), age, cigarette consumption per day (cigsPerDay), history of stroke (prevalentStroke), increased systolic blood pressure (sysBP), and high glucose level (glucose) are suggested to be prominent risk factors for CHD. With these features, the logistic regression gives the highest model accuracy of 85.97%.</p>


              
          </article>
          

<ul class="tags__list">
    
    </ul>

 <div class="pagination">
  
    <a class="pagination__item" href="/blog/eda/">
        <span class="pagination__label">Previous Post</span>
        <span class="pagination__title">üí° what is exploratory data analysis?</span>
    </a>
  

  
    <a class="pagination__item" href="/blog/custom_domain/">
      <span class="pagination__label">Next Post</span>
      <span class="pagination__title" >üîñ How to add a custom domain to your hugo site?</a>
    </a>
  
</div>

          
          <footer class="post__footer">
            


<div class="social-icons">
  
     
    
      <a class="social-icons__link" title="LinkedIn"
         href="https://www.linkedin.com/in/jessie-zheng612/"
         target="_blank" rel="noopener">
        <div class="social-icons__icon" style="background-image: url('svg/linkedin.svg')"></div>
      </a>
    
  
     
    
      <a class="social-icons__link" title="GitHub"
         href="https://github.com/Zheng612"
         target="_blank" rel="noopener">
        <div class="social-icons__icon" style="background-image: url('svg/github.svg')"></div>
      </a>
    
  
     
    
      <a class="social-icons__link" title="Email"
         href="mailto:Zheng612@umn.edu"
         target="_blank" rel="noopener">
        <div class="social-icons__icon" style="background-image: url('svg/email.svg')"></div>
      </a>
    
  
     
    
      <a class="social-icons__link" title="Instagram"
         href="https://www.instagram.com/manyu1201/"
         target="_blank" rel="noopener">
        <div class="social-icons__icon" style="background-image: url('svg/instagram.svg')"></div>
      </a>
    
     
</div>

            <p></p>
          </footer>
          </div>
      </div>
      
    </div>
    

  </main>

   

  
  <script src="/js/index.min.49e4d8a384357d9b445b87371863419937ede9fa77737522ffb633073aebfa44.js" integrity="sha256-SeTYo4Q1fZtEW4c3GGNBmTft6fp3c3Ui/7YzBzrr&#43;kQ=" crossorigin="anonymous"></script>
  
  
  <script src="https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js"></script>

  
  <script src="https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js"
    data-autoloader-path="https://unpkg.com/prismjs@1.20.0/components/"></script>

  


</body>

</html>
